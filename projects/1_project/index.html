<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Efficient Algorithms for Linearly Solvable Markov Decision Processes | David Pérez Carrasco </title> <meta name="author" content="David Pérez Carrasco"> <meta name="description" content="Study on Linearly Solvable Markov Decision Processes, incorporating the development of novel embedding techniques and scalable solutions within this framework."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://davidperezcarrasco.github.io/projects/1_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">David</span> Pérez Carrasco </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="text-align: justify;">Efficient Algorithms for Linearly Solvable Markov Decision Processes</h1> <p class="post-description" style="text-align: justify;">Study on Linearly Solvable Markov Decision Processes, incorporating the development of novel embedding techniques and scalable solutions within this framework.</p> </header> <article class="text-justify"> <p>As part of my Bachelor’s Thesis, I conducted in-depth research on reinforcement learning (RL) algorithms, addressing their scalability and efficiency in large and complex domains. This thesis explores the application of Linearly Solvable Markov Decision Processes (LMDPs) to tackle these challenges. A significant contribution of my work is the development of novel embedding techniques for creating precise and exact mappings between traditional MDPs and LMDPs. These techniques improve the approximation precision by 99.23% over the approach by Todorov, enabling robust and reliable comparisons across the two frameworks, regardless of the problem’s definition or the nature of its dynamics.</p> <p>This research evaluates and benchmarks the performance of traditional RL models against algorithms leveraging LMDPs, such as Z-learning, within an adaptable reinforcement learning framework. By implementing scalable and efficient versions of these algorithms, I provide a comprehensive comparison that highlights the advantages of LMDP-based approaches. Additionally, the thesis delves into various factors that enhance the decision-making capabilities of RL agents, such as algorithm design and exploration strategies, demonstrating the superiority of the LMDP framework in multiple settings. Moreover, I developed an <a href="https://github.com/davidperezcarrasco/Efficient-Algorithms-for-Linearly-Solvable-Markov-Decision-Processes" rel="external nofollow noopener" target="_blank">RL simulator</a> that provides a generalized implementation of MDP and LMDP frameworks. This simulator supports optimized core methods and seamless integration with repositories like Minigrid and Gymnasium, allowing researchers to easily extend and apply it to diverse problem definitions without the need to build custom solutions.</p> <h2 id="linearly-solvable-markov-decision-processes">Linearly Solvable Markov Decision Processes</h2> <p>LMDPs represent a specialized subclass of MDPs that achieve more efficient computation of optimal policies and value functions by leveraging linear programming techniques. Unlike traditional MDPs that solve nonlinear Bellman equations, LMDPs reformulate these equations into a linear form, significantly enhancing computational efficiency and scalability.</p> <p>An LMDP is defined by a tuple ( \langle \mathcal{S}, \mathcal{P}, \mathcal{R} \rangle ) where:</p> <ul> <li>( \mathcal{S} ) is a set of states.</li> <li> <table> <tbody> <tr> <td>( \mathcal{P}(s’</td> <td>s) ) is an uncontrolled transition probability distribution.</td> </tr> </tbody> </table> </li> <li>( \mathcal{R}(s) ) is an expected reward function.</li> </ul> <p>The controlled transition probabilities are defined as: [ \mathcal{P}<em>{\mathbf{u}}(s’ | s) = \mathcal{P}(s’ | s) e^{u</em>{s’}} ]</p> <p>The optimality Bellman equation, transformed using the exponential function, is: [ z(s) = e^{\mathcal{R}(s)/\lambda} \sum_{s’ \in \mathcal{S}} \mathcal{P}(s’ | s) z(s’) ]</p> <p>This can be written in matrix form as: [ \mathbf{z} = G\mathcal{P}\mathbf{z} ]</p> <p>Z-learning, an on-line learning algorithm for LMDPs, iteratively updates the value of ( \mathbf{z} ) based on observed transitions and rewards: [ \hat{z}(s_t) \gets (1 - \alpha)\hat{z}(s_t) + \alpha e^{r_t/\lambda}\hat{z}(s_{t+1}) ]</p> <h3 id="z-learning-algorithm">Z-learning Algorithm</h3> <p>\begin{algorithm} \caption{Z-learning} \label{alg:z-learning} \begin{algorithmic}[1] \State \textbf{input:} learning rate ( \alpha \in (0,1] ), temperature parameter ( \lambda &gt; 0 ), LMDP with ( \mathcal{R} ), ( \mathcal{P} ), ( \mathcal{S} ), ( \mathcal{S}^- ), ( \mathcal{T} ) \State \textbf{output:} ( \hat{Z}: S \rightarrow \mathbb{R} ) \State \textbf{initialize} ( \hat{Z}(s) \leftarrow 1 ) for all ( s \in \mathcal{S}^- ), ( \hat{Z}(s) \leftarrow e^{\mathcal{R}(s)/\lambda} ) for all ( s \in \mathcal{T} ), ( \hat{\mathcal{P}<em>{\mathbf{u}}} \leftarrow \mathcal{P} ) \Repeat \State ( s_t \gets s_0 ) (sample state from initial state distribution) \While{( s_t \notin \mathcal{T} )} \State Take reward ( r_t ) from the current state ( s_t ). \State ( G<a href="s_t">z</a> \leftarrow \sum</em>{s’ \in \mathcal{S}} \mathcal{P}(s’ \mid s)\hat{Z}(s’) ) \State ( \hat{Z}(s_t) \leftarrow \hat{Z}(s_t) + \alpha [ e^{r_t/\lambda} G<a href="s_t">z</a> - \hat{Z}(s_t) ] ) \State Update ( \hat{\mathcal{P}<em>{\mathbf{u}}} ) derived from ( \hat{Z} ) \State Sample a next state ( s</em>{t+1} ) according to ( \hat{\mathcal{P}<em>{\mathbf{u}}} ) \State ( s_t \leftarrow s</em>{t+1} ) \EndWhile \Until{convergence} \end{algorithmic} \end{algorithm}</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/minigrid_plots2-480.webp 480w,/assets/img/minigrid_plots2-800.webp 800w,/assets/img/minigrid_plots2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/minigrid_plots2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Throughput Comparison between Z Learning for a LMDP and Q Learning for an embedded MDP" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/value_function3-480.webp 480w,/assets/img/value_function3-800.webp 800w,/assets/img/value_function3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/value_function3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Optimal value function of a 4x4 Grid" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> On the left, a throughput benchmarking of Z Learning and Q Learning using the proper embedding from LMDP to MDP for precise comparison. On the right, the value function of the MDP for a small grid environment of 5 x 5 cells. </div> <p>My research focused on enhancing the performance and scalability of state-of-the-art RL algorithms like Q-Learning and Z-Learning. By applying these algorithms to challenging Minigrid environments, I explored and evaluated methods to improve their efficacy in handling complex decision-making tasks. This involved developing efficient computational techniques for optimal action selection and optimizing value function approximation within linear LMDPs. Furthermore, I investigated the impact of exploration decay rates on the performance and convergence of these algorithms.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 David Pérez Carrasco. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>