<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Efficient Algorithms for Linearly Solvable Markov Decision Processes | David Pérez Carrasco </title> <meta name="author" content="David Pérez Carrasco"> <meta name="description" content="Study of Linearly Solvable Markov Decision Processes, incorporating novel embedding techniques and scalable solutions."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://davidperezcarrasco.github.io/projects/1_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">David</span> Pérez Carrasco </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="text-align: justify;">Efficient Algorithms for Linearly Solvable Markov Decision Processes</h1> <p class="post-description" style="text-align: justify;">Study of Linearly Solvable Markov Decision Processes, incorporating novel embedding techniques and scalable solutions.</p> </header> <article class="text-justify"> <p>As part of my Bachelor’s Thesis, I conducted in-depth research on reinforcement learning (RL) algorithms, addressing their scalability and efficiency in large and complex domains. This thesis explores the application of Linearly Solvable Markov Decision Processes (LMDPs) to tackle these challenges. A significant contribution of my work is the development of novel embedding techniques for creating precise and exact mappings between traditional MDPs and LMDPs. These techniques improve the approximation precision by 99.23% over the approach by Todorov, enabling robust and reliable comparisons across the two frameworks, regardless of the problem’s definition or the nature of its dynamics.</p> <p>This research evaluates and benchmarks the performance of traditional RL models against algorithms leveraging LMDPs, such as Z-learning, within an adaptable reinforcement learning framework. By implementing scalable and efficient versions of these algorithms, I provide a comprehensive comparison that highlights the advantages of LMDP-based approaches. Additionally, the thesis delves into various factors that enhance the decision-making capabilities of RL agents, such as algorithm design and exploration strategies, demonstrating the superiority of the LMDP framework in multiple settings. Moreover, I developed an <a href="https://github.com/davidperezcarrasco/Efficient-Algorithms-for-Linearly-Solvable-Markov-Decision-Processes" rel="external nofollow noopener" target="_blank">RL simulator</a> that provides a generalized implementation of MDP and LMDP frameworks. This simulator supports optimized core methods and seamless integration with repositories like Minigrid and Gymnasium, allowing researchers to easily extend and apply it to diverse problem definitions without the need to build custom solutions.</p> <h2 id="linearly-solvable-markov-decision-processes">Linearly Solvable Markov Decision Processes</h2> <p>LMDPs are defined as \(\mathcal{L} = (\mathcal{S}, \mathcal{S}^-, \mathcal{T}, \mathcal{P}, \mathcal{R}, \lambda)\) where:</p> <ul> <li>\(\mathcal{S}^-\): Set of non-terminal states.</li> <li>\(\mathcal{T}\): Set of terminal states.</li> <li>\(\mathcal{S}: \mathcal{S}^- \cup \mathcal{T}\): Set of states.</li> <li>\(\mathcal{P}: \mathcal{S}^- \to \Delta(\mathcal{S})\): Passive dynamics.</li> <li>\(\mathcal{R}: \mathcal{S} \to \mathbb{R}\): Reward function.</li> <li>\(\lambda\): Temperature parameter.</li> </ul> <p>The optimality Bellman equation for LMDPs is:</p> \[\frac{1}{\lambda} v(s) = \frac{1}{\lambda} \mathcal{R}(s) + \log{\mathcal{G}_{z}(s)} - \min_{\mathbf{u} \in \mathcal{U}(s)} KL \left( \mathcal{P}_{\mathbf{u}}( \cdot | s) \bigg\Vert \frac{\mathcal{P}(\cdot | s) z(\cdot)}{\mathcal{G}_{z}(s)}\right) \quad \forall s \in \mathcal{S}^-\] <p>where \(z(s) = e^{\frac{v(s)}{\lambda}}\) and \(\mathcal{G}_{z}(s) = \sum_{s' \in \mathcal{S}} \mathcal{P}(s' \mid s) z(s')\) with \(v(s) = \mathcal{R}(s) \quad \forall s \in \mathcal{T}\).</p> <p>Using this exponential transformation, the Bellman equation can be reformulated as:</p> \[z(s) = e^{\mathcal{R}(s)/\lambda} \sum_{s' \in \mathcal{S}} \mathcal{P}(s' | s) z(s')\] <p>In matrix form, this is expressed as:</p> \[\mathbf{z} = G\mathcal{P}\mathbf{z}\] <p>where the vector \(\mathbf{z}\) consists of elements \(z(s)\), and the diagonal matrix \(G\) has terms \(e^{\mathcal{R}(s)/\lambda}\) on its main diagonal.</p> <p>The optimally controlled transition probabilities can be obtained using the following equation:</p> \[\mathcal{P}_{\mathbf{u}^*}(s' | s) = \frac{\mathcal{P}(s' | s) z(s')}{\sum_{s'' \in \mathcal{S}} \mathcal{P}(s'' | s) z(s'')}\] <p>The iterative method to solve the final matrix equation is Power Iteration, which is equivalent to Value Iteration in MDPs. However, when a model of the environment is not available, an online algorithm becomes necessary. The analogous online algorithm to Q-learning in LMDPs is Z-learning, introduced by <a href="#todorov-2006">Todorov (2006)</a>:</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/power-iteration-alg-480.webp 480w,/assets/img/lmdps/power-iteration-alg-800.webp 800w,/assets/img/lmdps/power-iteration-alg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/power-iteration-alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Power Iteration Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Power Iteration Algorithm </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/z-learning-alg-480.webp 480w,/assets/img/lmdps/z-learning-alg-800.webp 800w,/assets/img/lmdps/z-learning-alg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/z-learning-alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Z-learning Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Z-learning Algorithm </div> </div> </div> <h2 id="methodology">Methodology</h2> <h3 id="embedding-stochastic-mdps-into-lmdps">Embedding Stochastic MDPs into LMDPs</h3> <p>The embedding of stochastic MDPs into LMDPs involves the following system of \(\vert\mathcal{A}\vert\) equations for a fixed state \(s\):</p> \[\begin{aligned} m_{s'} &amp;= \log{\mathcal{P}(s' | s)} \\ b_a &amp;= \tilde{\mathcal{R}}(s,a) + \sum_{s' \in \mathcal{S}} \tilde{\mathcal{P}}(s' | s, a) \log{\tilde{\mathcal{P}}(s' | s, a)} \\ D_{as'} &amp;= \tilde{\mathcal{P}}(s' | s, a) \end{aligned}\] <p>By leveraging the stochasticity of matrix \(D\) (resulting in \(D\mathbf{1} = \mathbf{1}\)), this system can be expressed in matrix form as:</p> \[D\left(\mathcal{R}\mathbf{1} + \mathbf{m}\right) = \mathbf{b}\] <p>This linear system can be solved for \(\mathbf{c} = \mathcal{R}\mathbf{1} + \mathbf{m}\) using a linear solver, resulting in:</p> \[\mathbf{m} = \mathbf{c} - \mathcal{R}\mathbf{1}\] <p>This formulation allows for flexibility in the choice of \(\mathcal{R}\). However, to ensure that the uncontrolled transition probabilities \(\mathcal{P}\) are normalized, we define:</p> \[\mathcal{R} = \log{\sum_{s' \in \mathcal{S}}e^{-c_{s'}}}\] <p>We then find the corresponding \(\mathbf{m}\) using this normalization condition, thereby ensuring that the solution adheres to the probabilistic constraints of the LMDP framework. The entire embedding process from <a href="#todorov-2006">Todorov (2006)</a> has been implemented in a vectorized manner, significantly enhancing the efficiency and scalability of the methodology.</p> <div class="row justify-content-sm-center"> <div class="col-sm-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/stochastic-mdp-embedding-480.webp 480w,/assets/img/lmdps/stochastic-mdp-embedding-800.webp 800w,/assets/img/lmdps/stochastic-mdp-embedding-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/stochastic-mdp-embedding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Embedding of stochastic MDP into LMDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Embedding of stochastic MDP into LMDP implementation </div> <h3 id="embedding-deterministic-mdps-into-lmdps">Embedding Deterministic MDPs into LMDPs</h3> <p>When the MDP is deterministic, the previous embedding technique cannot be performed, as there is no entropy within the transition probability distribution. This makes the exact construction of an LMDP infeasible. Todorov’s approach can still be used by removing the entropy factor and scaling out the rewards when necessary, but it leads to suboptimal approximations. Therefore, an alternative methodology has been proposed in this work, presenting a novel approach for accurately and efficiently constructing the most precise possible LMDP from a deterministic MDP, outperforming previous baselines in embedding precision and robustness.</p> <p>The first alternative method involves considering a stochastic “policy” to translate the MDP to an LMDP. This method results in:</p> \[\mathcal{R}(s) \gets \frac{1}{\left|\mathcal{A}\right|}\sum_{a \in \mathcal{A}} \tilde{\mathcal{R}}(s,a), \quad \forall s \in \mathcal{S}\] \[\mathcal{P}(s' | s) \gets \frac{1}{\left|\mathcal{A}\right|}\sum_{a \in \mathcal{A}} \tilde{\mathcal{P}}(s' | s,a), \quad \forall s \in \mathcal{S}^-, s' \in \mathcal{S}\] <p>However, the equivalence between the MDP and LMDP rewards must consider the Kullback-Leibler’s divergence as:</p> \[\tilde{\mathcal{R}}(s,a) = \mathcal{R}(s) - \lambda KL \left( \mathcal{P}_{\textbf{u}} \Vert \mathcal{P} \right)\] <p>To apply this in the desired direction, the LMDP dynamics must be known, which do not hold in this case as we are trying to construct the LMDP. An alternative method is to use the dynamics defined through the stochastic policy averaging (SPA) method, obtaining \(\mathcal{P}_{\textbf{u}}\) from these dynamics, and then updating \(\mathcal{R}\).</p> <div class="row justify-content-sm-center"> <div class="col-sm-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/deterministic-mdp-embedding-spa-480.webp 480w,/assets/img/lmdps/deterministic-mdp-embedding-spa-800.webp 800w,/assets/img/lmdps/deterministic-mdp-embedding-spa-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/deterministic-mdp-embedding-spa.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Embedding of deterministic MDP into LMDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Embedding of deterministic MDP into LMDP implementation through SPA method </div> <p>Initial intuition might suggest converting this into an iterative process. However, empirical experimentation across multiple settings revealed that the optimal approximation was consistently achieved in the very first iteration. Consequently, we hypothesize that there exists a factor \(K\) such that the optimal approximation of the embedded LMDP reward can be expressed as:</p> \[\mathcal{R}(s) = K \cdot \hat{\mathcal{R}}(s), \quad \forall s \in \mathcal{S}.\] <p>Where \(\hat{\mathcal{R}}(s)\) is the initial approximation of \(\mathcal{R}\). The optimal value of the factor \(K\) can be determined using a search algorithm, with the objective function being the Mean Squared Error (MSE) between the value function approximation of the embedded LMDP and the original MDP. This minimization problem can be expressed as follows:</p> \[\begin{aligned} \min_{K \in \mathbb{R}} \quad \quad &amp; \left\| \mathbf{v}_K - \mathbf{v}^* \right\|^2 \\ \text{subject to} \quad \quad &amp; \mathbf{\mathcal{R}} = K \cdot \mathbf{\hat{\mathcal{R}}}, \\ &amp; G_{ii} = e^{\mathcal{R}(s_i)/\lambda}, \quad \forall i \in \mathbb{N} \cap [1, |\mathcal{S}|], \\ &amp; \mathbf{z}_K = G\mathcal{P}\mathbf{z}_K, \\ &amp; \mathbf{v}_K = \lambda \log{\mathbf{z}_K}, \\ &amp; \mathbf{v}^* = \max_{a} \left[ \tilde{\mathbf{R}}_a + \gamma \mathbf{\tilde{P}}_a \mathbf{v}^* \right]. \end{aligned}\] <p>Binary Search (BS) can be employed to solve this minimization problem. However, although \(\mathbf{v}_K\) is a monotonically increasing function, the objective function is unimodal, making Ternary Search (TS) marginally more suitable for this problem.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/deterministic-mdp-embedding-ts-480.webp 480w,/assets/img/lmdps/deterministic-mdp-embedding-ts-800.webp 800w,/assets/img/lmdps/deterministic-mdp-embedding-ts-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/deterministic-mdp-embedding-ts.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Embedding of deterministic MDP into LMDP" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Embedding of deterministic MDP into LMDP implementation through TS method </div> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/deterministic-mdp-embedding-scatter-480.webp 480w,/assets/img/lmdps/deterministic-mdp-embedding-scatter-800.webp 800w,/assets/img/lmdps/deterministic-mdp-embedding-scatter-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/deterministic-mdp-embedding-scatter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Deterministic MDP Embedding Methodologies Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Deterministic MDP Embedding Approximations Comparison </div> <p>All TS, BS and SPA methods outperformed the baseline’s Todorov’s Embedding (TE) method, with TS and BS reaching the same \(R^2\), but a marginally better MSE for TS (\(0.1525\) against \(0.1527\)). TE achieved an MSE of \(19.8\), meaning our novel approach outperforms Todorov’s embedding precision for a deterministic MDP by \(99.23\%\). This difference is pronounced in larger settings, where our method still performs considerably well with an \(R^2=0.9917\) while TE decays to \(R^2=0.1996\), a significant difference. Furthermore, TS maintains consistent high precision while increasing the problem’s size, while TE’s accuracy diminishes significantly, showcasing the scalability and robustness of our method.</p> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/deterministic-mdp-embedding-scatter-large-480.webp 480w,/assets/img/lmdps/deterministic-mdp-embedding-scatter-large-800.webp 800w,/assets/img/lmdps/deterministic-mdp-embedding-scatter-large-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/deterministic-mdp-embedding-scatter-large.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="PDeterministic MDP Embedding Methodologies Comparison in large settings" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Deterministic MDP Embedding Approximations Comparison in large settings </div> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/deterministic-mdp-embedding-mse-480.webp 480w,/assets/img/lmdps/deterministic-mdp-embedding-mse-800.webp 800w,/assets/img/lmdps/deterministic-mdp-embedding-mse-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/deterministic-mdp-embedding-mse" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Deterministic MDP Embedding Methodologies MSE comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Deterministic MDP Embedding Approximations MSE Comparison by problem size </div> <h2 id="experimentation">Experimentation</h2> <h3 id="exploration-strategies-in-traditional-mdps">Exploration Strategies in Traditional MDPs</h3> <div class="row justify-content-sm-center"> <div class="col-sm-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/q-learning-epsilon-decay-480.webp 480w,/assets/img/lmdps/q-learning-epsilon-decay-800.webp 800w,/assets/img/lmdps/q-learning-epsilon-decay-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/q-learning-epsilon-decay.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Q-learning convergence and approximation MSE by exploration decay" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Q-learning convergence and approximation MSE by exploration decay </div> <h3 id="comparative-analysis-of-z-learning-and-q-learning">Comparative Analysis of Z-learning and Q-learning</h3> <h4 id="approximation-error-and-episodic-reward">Approximation Error and Episodic Reward</h4> <div class="row justify-content-sm-center"> <div class="col-sm-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/zvsq-rew-err-480.webp 480w,/assets/img/lmdps/zvsq-rew-err-800.webp 800w,/assets/img/lmdps/zvsq-rew-err-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/zvsq-rew-err.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Comparison of approximation error (top) and episodic reward (bottom) between Q-learning and Z-learning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of approximation error (top) and episodic reward (bottom) between Q-learning and Z-learning. </div> <h4 id="value-function-and-policy-approximations">Value Function and Policy Approximations</h4> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/zvsq-val-multiroom-480.webp 480w,/assets/img/lmdps/zvsq-val-multiroom-800.webp 800w,/assets/img/lmdps/zvsq-val-multiroom-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/zvsq-val-multiroom.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Value function and policy approximations for Q-learning and Z-learning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Value function and policy approximations for Q-learning and Z-learning in a Minigrid multi-room domain </div> <div class="row justify-content-sm-center"> <div class="col-sm-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lmdps/zvsq-val-hill-480.webp 480w,/assets/img/lmdps/zvsq-val-hill-800.webp 800w,/assets/img/lmdps/zvsq-val-hill-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/lmdps/zvsq-val-hill.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Value function and policy approximations for Q-learning and Z-learning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Value function and policy approximations for Q-learning and Z-learning in a Minigrid hill-cliff domain. </div> <p>Z-learning significantly outperforms Q-learning by converging nearly 10 times faster, demonstrating superior efficiency and computational cost reduction. This efficiency is particularly notable in environments with high state-space complexity. However, inaccuracies in policy approximation for both methods arise from the lack of random sampling, as the tasks are designed to create goal-oriented problems, making random sampling impractical and less effective for these settings.</p> <h4 id="agents-solutions">Agents Solutions</h4> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <video src="/assets/video/lmdps/small-maze-lmdp.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Grid World Maze with Q-learning" autoplay="" controls=""></video> </figure> <div class="caption"> Grid World Maze with Q-learning </div> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <video src="/assets/video/lmdps/large-maze-mdp.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Large Grid World Maze with Q-learning" autoplay="" controls=""></video> </figure> <div class="caption"> Large Grid World Maze with Q-learning </div> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <video src="/assets/video/lmdps/hill.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Grid World Hill-Cliff with Q-learning" autoplay="" controls=""></video> </figure> <div class="caption"> Grid World Hill-Cliff with Q-learning </div> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <video src="/assets/video/lmdps/multi-room-lmdp.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Grid World Multi-Room with Z-learning" autoplay="" controls=""></video> </figure> <div class="caption"> Grid World Multi-Room with Z-learning </div> </div> </div> <h2 id="thesis-report">Thesis Report</h2> <div class="row justify-content-sm-center"> <object data="/assets/pdf/lmdps/thesis-report.pdf" width="600" height="800" type="application/pdf"></object> </div> <h2 id="thesis-presentation">Thesis presentation</h2> <div class="row justify-content-sm-center"> <object data="/assets/pdf/lmdps/thesis-presentation.pdf" width="600" height="500" type="application/pdf"></object> </div> <h2 id="references">References</h2> <p>[Gómez et al., 2014] Gómez, V., Kappen, H. J., Peters, J., and Neumann, G. (2014). Policy search for path integral control. In Calders, T., Esposito, F., Hüllermeier, E., and Meo, R., editors</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 David Pérez Carrasco. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>